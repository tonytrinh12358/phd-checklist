# Research Idea Evaluation Checklist for AI & CV Papers

## 1. Problem Definition & Clarity
- [ ] **Clear Objective**: What is the ultimate objective of your research?
- [ ] **Output Specification**: What exactly will the model produce? (e.g., images, 3D models, classifications, etc.)
- [ ] **Problem Boundaries**: Is the problem scope well-defined and neither too broad nor too narrow?
- [ ] **Concrete Examples**: Can you provide specific input-output examples that illustrate your problem?

## 2. Research Positioning & Novelty
- [ ] **Literature Mapping**: Can you map your problem to existing research areas/standard problems?
- [ ] **Novel Contribution**: What makes your approach different from existing work?
- [ ] **Abstraction Level**: Have you identified the right level of abstraction that unifies your contributions?
- [ ] **Gap Identification**: What specific gap in current research are you addressing?

## 3. Technical Feasibility Assessment
- [ ] **Computational Constraints**: Is the proposed approach computationally feasible?
- [ ] **Model Limitations**: Have you considered inherent limitations of current models (e.g., precision, numerical understanding)?
- [ ] **Technical Requirements**: Do you have access to necessary computational resources and expertise?
- [ ] **Proof of Concept**: Can you demonstrate initial feasibility through preliminary experiments?

## 4. Evaluation Strategy
- [ ] **Evaluation Metrics**: How will you quantitatively evaluate your approach?
- [ ] **Baseline Comparison**: What existing methods will serve as baselines?
- [ ] **Dataset Availability**: Are there standard datasets for evaluation, or can existing datasets be adapted?
- [ ] **Ground Truth**: Is ground truth data available or can it be created/annotated?
- [ ] **Evaluation Protocols**: Have you defined clear, reproducible evaluation procedures?

## 5. Technical Contribution Design
- [ ] **Core Innovation**: What is your main technical contribution? (e.g., new architecture, loss function, training strategy)
- [ ] **Improvement Mechanism**: How specifically does your approach improve upon baselines?
- [ ] **Contribution Substantiality**: Is the contribution significant enough for your target venue?
- [ ] **Multiple Angles**: Have you considered different technical approaches to solve the problem?

## 6. Research vs Application Balance
- [ ] **Fundamental Research Value**: Does the work contribute to fundamental understanding beyond the specific application?
- [ ] **Generalizability**: Can your approach be applied to other problems/domains?
- [ ] **Real-world Relevance**: Does it address actual needs while maintaining research rigor?
- [ ] **Backwards Design**: Have you worked backwards from the desired outcome to define research questions?

## 7. Implementation Strategy
- [ ] **Modular Design**: Can the problem be broken into manageable sub-problems?
- [ ] **Pipeline vs End-to-End**: Have you considered both approaches and chosen appropriately?
- [ ] **Incremental Development**: Can you build the solution incrementally with measurable progress?
- [ ] **Risk Mitigation**: Have you identified high-risk components and backup plans?

## 8. Dataset & Resource Planning
- [ ] **Data Requirements**: What type and amount of data do you need?
- [ ] **Data Availability**: Are suitable datasets publicly available?
- [ ] **Annotation Needs**: Will you need to create new annotations or datasets?
- [ ] **Computational Resources**: Do you have access to necessary GPU/TPU resources?

## 9. Paper Design & Presentation
- [ ] **Story Arc**: Can you articulate a compelling narrative from problem to solution?
- [ ] **Target Venue**: Which conference/journal is most suitable for this work?
- [ ] **Related Work Positioning**: How will you position your work relative to existing literature?
- [ ] **Reproducibility**: Can others reproduce your results based on your description?

## 10. Risk Assessment & Alternatives
- [ ] **Feasibility Risks**: What are the main technical risks and how will you address them?
- [ ] **Timeline Realism**: Is the project achievable within your timeline?
- [ ] **Pivot Options**: If the main approach fails, what alternatives exist?
- [ ] **Minimum Viable Contribution**: What's the minimum result that would still be publishable?

## Quick Evaluation Questions
1. **One-sentence summary**: Can you explain your research in one clear sentence?
2. **Grandma test**: Can you explain why this matters to a non-technical person?
3. **Conference fit**: Which top conference would be excited about this work?
4. **Failure modes**: What would prevent this from working and how likely are they?
5. **Six-month check**: Will you still be excited about this problem in six months?

## Red Flags to Avoid
- ❌ Problem is too application-specific without research depth
- ❌ No clear evaluation strategy or metrics
- ❌ Contribution is incremental or unclear
- ❌ Requires resources/data that are unavailable
- ❌ Too similar to existing work without significant improvement
- ❌ Overly ambitious scope without feasible milestones

## Green Flags to Pursue
- ✅ Clear problem definition with concrete examples
- ✅ Novel angle on an important problem
- ✅ Feasible with available resources
- ✅ Clear evaluation plan with accessible datasets
- ✅ Strong theoretical or practical motivation
- ✅ Potential for broader impact beyond specific application